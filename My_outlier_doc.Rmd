---
title: "Outlier_tests"
author: "Koppu Aditya"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_tex: true
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, warning=FALSE,include=F}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(dplyr, tibble, tidyr, ggplot2,magrittr,EnvStats, car, skimr, broom, janitor, readr)

```

## Getting to know data
* Melborn data from Kaggle: https://www.kaggle.com/datasets/anthonypino/melbourne-housing-market?select=Melbourne_housing_FULL.csv

```{r data_loading , message=FALSE, warning=FALSE}
Melbourn = read_csv("./Data/Melbourne_housing_FULL.csv")
```
### Data Exploration

```{r exploring data, message=F, warning=F}
skim(Melbourn)
```
### Applying Basic filters
```{r features_melbourn}
Melbourn %<>% select(which(colMeans(!is.na(Melbourn)) > 0.5)) %<>% filter( !(is.na(Price)) , Price > 10, !is.na(Landsize), Landsize > 10) %<>%
  mutate('Price_per_area' = Price/Landsize) %<>% filter(Price_per_area < 15000)

```

### Univariate plots

```{r plots_melbourn}
Melbourn %>% ggplot(aes(x=Regionname, y = Price_per_area))+geom_boxplot()+theme(axis.text.x = element_text(angle = 45, hjust=1))

Melbourn %>% ggplot(aes(x=Regionname, y = log(Landsize)))+geom_boxplot()+theme(axis.text.x = element_text(angle = 45, hjust=1))

Melbourn %>% ggplot(aes(x= Price_per_area))+geom_histogram(bins= 50)

```






## Simple Statistical Univariate rules

### Z score

The standard score or Z-score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. 

Formula is $$ z = \frac{x -\overline{x}}{\sigma} $$
where $\overline{x}$ is mean of obs and $\sigma$ is sd of obs

Usually anything more than 2 times the sd away is an outlier in data. 

```{r, echo=FALSE}
# Define variable containing url
url <- "https://upload.wikimedia.org/wikipedia/commons/2/25/The_Normal_Distribution.svg"
```
#### Normal distribution
<center><img src="`r url`"></center>

```{r Z_score}

Melbourn %<>% mutate("Z_score" = (Price_per_area - mean(Price_per_area))/sd(Price_per_area), "Z_score_grt_than_2" = as.numeric(Z_score > 2))

Melbourn %>% ggplot(aes(x= Price_per_area, fill = as.factor(Z_score_grt_than_2)))+geom_histogram(bins= 50)+labs(fill = "Z score > 2")

Melbourn %>% mutate('Z_score_' = ifelse(Z_score > 2,  "> 2", "<= 2") ) %>%  tabyl(Regionname, Z_score_) %>%  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>% 
  adorn_pct_formatting(rounding = "half up", digits = 0) %>%
  adorn_ns() %>%
  adorn_title("combined")%>%
  knitr::kable()
```

### InterQuartile Range(IQR)
 $$ IQR = Q_3 - Q_1$$
 where given 2n/2n+1 numbers
 
 
* first quartile $Q_1$ Median of n smallest values
* third quartile $Q_3$ Median of n largest values

Usually $Q_1 +/- (1.5*IQR)$ is considered to be an outlier. In our case, we considered $2*IQR$ as outliers. 
       
```{r, echo=FALSE}
# Define variable containing url
url <- "https://upload.wikimedia.org/wikipedia/commons/1/1a/Boxplot_vs_PDF.svg"
```

<center><img src="`r url`"></center> 


```{r IQR_calc}
lowerq = quantile(Melbourn$Price_per_area)[2]
upperq = quantile(Melbourn$Price_per_area)[4]
iqr = upperq -lowerq
threshold.upper = (iqr*2) + upperq
threshold.lower = lowerq - (iqr*2)

Melbourn %>% mutate("Outside_threshold" = ifelse(Price_per_area > threshold.upper | Price_per_area < threshold.lower , 1, 0)) %>% ggplot(aes(x= Price_per_area, fill = as.factor(Outside_threshold)))+geom_histogram(bins= 50)+labs(fill = "Outside 2*IQR")

Melbourn %>% mutate('IQR_' = ifelse(Price_per_area > threshold.upper | Price_per_area < threshold.lower , ">2", "<2")) %>%  tabyl(Regionname, IQR_) %>%  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>% 
  adorn_pct_formatting(rounding = "half up", digits = 0) %>%
  adorn_ns() %>%
  adorn_title("combined")%>%
  knitr::kable()

```

### Hamples filter

If a point exceeds the sum of median and 3*(Median Absolute deviation) then it is considered an outlier. This method is usually used in timeseries by selecting a window size to be considered for sliding window.

$$ outlier = median +/- 3*MAD$$

where $$MAD = \frac{1}{n} \Sigma|x_i - median(X)|$$

## Statistical tests

### Rosner's test

Details: https://alexkowa.github.io/EnvStats/reference/rosnerTest.html


```{r rosner_test}
test <- rosnerTest(x =c(Melbourn$Price_per_area),
                   k = 1000,
                   alpha = 0.05,
                   warn = F
)
Outlier_indices = as_tibble(test$all.stats) %>% filter(Outlier == TRUE) %>% select(Obs.Num)

Melbourn %>% mutate("rosner_outlier" =  ifelse(row_number() %in% Outlier_indices$Obs.Num,1,0)) %>% ggplot(aes(x= Price_per_area, fill = as.factor(rosner_outlier)))+geom_histogram(bins= 50)+labs(fill = "rosner_outlier")

Melbourn %>% mutate("rosner_outlier" =  ifelse(row_number() %in% Outlier_indices$Obs.Num,1,0)) %>%  tabyl(Regionname, rosner_outlier) %>%  adorn_totals(c("row", "col")) %>%
  adorn_percentages("row") %>% 
  adorn_pct_formatting(rounding = "half up", digits = 0) %>%
  adorn_ns() %>%
  adorn_title("combined")%>%
  knitr::kable()



```


```{r feature_engg}
#Building quick regression. Check missing values
Melbourn.model.df = Melbourn %>% mutate('Car_per_Bedrooms' = Car/Bedroom2, 'Car_per_Bedrooms' = ifelse(is.infinite(Car_per_Bedrooms) | is.na(Car_per_Bedrooms), median(Car_per_Bedrooms, na.rm = T), Car_per_Bedrooms), 'Rooms_per_Landsize' = Rooms/Landsize, 'Date' = lubridate::dmy(Date), 'Year_sold' = lubridate::year(Date))%>% select(-c("Address", "Price", "Lattitude", "Longtitude", "Postcode", "Z_score_grt_than_2", "Z_score", "Bedroom2", "Car")) %>% filter(!(is.na(Bathroom))) %>% mutate_if(is.character, as.factor) %>% select(-c("Suburb", "SellerG"))

skim(Melbourn.model.df)

```

```{r,echo=F}
# set.seed(456)
# boruta2 <- Boruta(Price_per_area~., data = Melbourn.model.df, doTrace = 1)
#print(boruta2)
#plot(boruta2)

```

```{r, echo=F}
# plot(boruta2, xlab = "", xaxt = "n")
# k <-lapply(1:ncol(boruta2$ImpHistory),function(i)
#   boruta2$ImpHistory[is.finite(boruta2$ImpHistory[,i]),i])
# names(k) <- colnames(boruta2$ImpHistory)
# Labels <- sort(sapply(k,median))
# axis(side = 1,las=2,labels = names(Labels),
#        at = 1:ncol(boruta2$ImpHistory), cex.axis = 0.7)
# 
# Impvars = attStats(boruta2) %>% arrange(-medianImp) %>% filter(medianImp > 30) %>%  rownames()
# finalvars = getSelectedAttributes(boruta2, withTentative = F) 

```

```{r,  echo=FALSE}
# pacman::p_load(caret,randomForest)
# set.seed(456)
# control <- rfeControl(functions= rfFuncs, method = "repeatedcv",repeats = 3, number=10)
# x = Melbourn.model.df %>% select(-c("Price_per_area"))
# y = Melbourn.model.df %>% select(c("Price_per_area")) %>% unlist() %>% unname %>% as.vector()
# nrow(x)
# class(y)
# rfe <- rfe(x,y,rfeControl=control)
# print(rfe, top=10)
# plot(rfe, type=c("g", "o"), cex = 1.0)
# predictors(rfe)
# head(rfe$resample, 10)

```

```{r, echo=FALSE}
# # ensure the results are repeatable
# set.seed(7)
# # load the library
# pacman::p_load(mlbench,caret)
# #library()
# # load the data
# data(PimaIndiansDiabetes)
# # define the control using a random forest selection function
# control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# # run the RFE algorithm
# results <- rfe(PimaIndiansDiabetes[,1:8], PimaIndiansDiabetes[,9], sizes=c(1:8), rfeControl=control)
# # summarize the results
# print(results)
# # list the chosen features
# predictors(results)
# # plot the results
# plot(results, type=c("g", "o"))

```


### Building LM to predict price per unit area

```{r basic_lm}
Melbourn.model.df %<>%  mutate(Landsize_log := log10(Landsize))
cols.idv = c("Distance"
             ,"Regionname"
             ,"Rooms_per_Landsize"
             ,"Bathroom"
             ,"Type" 
             #,"Propertycount"
             ,"Landsize_log"
  
)

mod <- lm(as.formula(paste0("Price_per_area~",paste0(cols.idv,collapse = "+" ))), data = Melbourn.model.df)

tidy(mod) %>% knitr::kable()
glance(mod) %>%  knitr::kable()

Melbourn.model.df %<>% mutate("predicted" =  predict(mod,new_data =Melbourn.model.df ), "residuals" = Price_per_area - predicted, "APE" = abs(residuals)/Price_per_area ) 

Melbourn.model.df %>% summarise('MAPE' = mean(APE), 'count' =n()) %>% knitr::kable()
```
## LM outlier tests

## Bonferroni Outlier test 

This comes from car Outlier_test
Details: https://search.r-project.org/CRAN/refmans/car/html/outlierTest.html

```{r Bonferroni}

outlierTest(mod) # 10 rows as outlier
outlier_rows = outlierTest(mod)$bonf.p %>% as.data.frame() %>% row.names() %>% as.numeric()
Melbourn.model.df %<>% mutate("bonferroni_outliers" = as.factor(row_number() %in% outlier_rows )) 

Melbourn.model.df %>% group_by(bonferroni_outliers) %>% summarise(mean(APE), 'count' =n()) %>% knitr::kable()


```

### Cooks distance

Cook's distance or Cook's D is a commonly used estimate of the influence of a data point when performing a least-squares regression analysis. So, these are not exactly outliers but are influential points. Checking for common features for these points before excluding them will be better. 

$$D_i = \frac{\Sigma{(\hat{Y_j} - \hat{Y_{j(i)}})}^2}{(p+1){\sigma}^2} $$
Where 

* $y_j$ — the jth fitted response value.
* $y_{j(i)}$ — the jth fitted response value, where the fit does not include observation i.
* $p$ — the number of regression coefficients
* $\sigma$ — the estimated variance from the fit, based on all observations, i.e. Mean Squared Error



```{r cooksd}
cooksD <- cooks.distance(mod)
influential <- as.numeric(names(cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]))

plot(cooksD)
```

The number of points that have > 3 mean are `r length(influential)`

```{r cooksd2}
Melbourn.model.df %<>% mutate("cooks_outliers1" = as.factor(row_number() %in% influential )) 
Melbourn.model.df %>% group_by(cooks_outliers1) %>% summarise(mean(APE), 'count' =n()) %>% knitr::kable()

```

```{r, echo=F}
# sample_size <- nrow(Melbourn.model.df)
# plot(cooksD, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
# abline(h = 4/sample_size, col="red")  # add cutoff line
# text(x=1:length(cooksD)+1, y=cooksD, labels=ifelse(cooksd>4/sample_size, names(cooksD),""), col="red")  # add labels
# 
# influential2 <- cooksD[(cooksD > 4/sample_size)]
# length(influential)
# 
# 
# Melbourn.model.df %>%  ggplot(aes(x = Landsize_log, y =Price_per_area ))+geom_point()+geom_smooth(method = lm) 
# 
# Melbourn.model.df %>% summarise(summary(Price_per_area))

# cols.idv = c("Distance"
#              ,"Regionname"
#              ,"Rooms_per_Landsize"
#              ,"Bathroom"
#              ,"Type" 
#              #,"Propertycount"
#              ,"Landsize_log"
#              , "cooks_outliers1"
#   
# )
# 
# mod <- lm(as.formula(paste0("Price_per_area~",paste0(cols.idv,collapse = "+" ))), data = Melbourn.model.df)
# 
# tidy(mod) %>% knitr::kable()
# glance(mod) %>%  knitr::kable()
# 
# Melbourn.model.df2 = Melbourn.model.df %>% mutate("predicted" =  predict(mod,new_data =Melbourn.model.df ), "residuals" = Price_per_area - predicted, "APE" = abs(residuals)/Price_per_area ) 
#   
# Melbourn.model.df2 %>%  summarise(mean(APE), 'count' =n())



```

### Isolation Forest

Tree based method. It works to isolate a point or set of identical points as quickly as possible. 

Sources: 
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html

https://medium.com/codex/isolation-forest-outlier-detection-simplified-5d938548bb5c




```{r}
p_load(isotree)

iso_forest =isolation.forest(Melbourn.model.df %>% select(all_of(c(cols.idv, "Price_per_area"))))

Melbourn.model.df$forest_pred <- predict(iso_forest, Melbourn.model.df, type = "score")

Melbourn.model.df %>%  ggplot(aes(x = Landsize_log, y =forest_pred ))+geom_point()

Melbourn.model.df %>% group_by(forest_pred >= 0.5) %>% summarise(mean(APE), 'count' =n())


```
